{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2886ee",
   "metadata": {},
   "source": [
    "# <font color='darkblue'> Part 3 : Smoking Network Classification using Graph Convolutional Networks</font> \n",
    "\n",
    "In this part we will work with a Graph Convolutional Network (GCN). \n",
    "\n",
    "In the previous part we formulated a task to predict whether individuals in the Generation Scotland Dataset have ever smoked. \n",
    "\n",
    "We generated a patient similarity network from CpG sites which are known to be associated with smoking. We will also work with the patient similarity network generated in section 1. \n",
    "\n",
    "In this part we postulate that individuals who have previously smoked will share similar epigenetic markers which we can learn from. \n",
    "We will now highlight the difference between our two networks to see if we can learn from them\n",
    "\n",
    "In this part we will be : \n",
    "- Working with Deep Graph Library\n",
    "- Building a GCN \n",
    "- Training a GCN\n",
    "- Comparing the performance of PSN's \n",
    "- Comparing the performance of GCN to simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75911ae1",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Import Functions and Packages </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0 , '/tutorial/')\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd1a4c",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Load Data and Network </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using pickle\n",
    "with open('/data/intermediate/GCN_Data.pkl' , 'rb') as file : \n",
    "    loaded_data = pd.read_pickle(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c253f",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Working with [Deep Graph Library](https://www.dgl.ai/)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "'''\n",
    "#################################################\n",
    "                YOUR CODE HERE\n",
    "#################################################\n",
    "Create a DGL graph from the networkx graph \n",
    "\n",
    "Add the features to the nodes, ensuring the order of the \n",
    "features matches the order of the nodes. \n",
    "\n",
    "Note : DGL only accepts torch.Tensors as feature attributes\n",
    "'''\n",
    "\n",
    "g = dgl.from_networkx(, node_attrs=[]) \n",
    "g.ndata['feat'] = torch.Tensor() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check that our graph is preserved\n",
    "phenotypes = loaded_data['phenotypes'] # Load the phenotypes\n",
    "\n",
    "node_0 = phenotypes.iloc[g.ndata['idx'].numpy()].iloc[0] # Get the idx of the first node of the dgl graph\n",
    "\n",
    "node_0_edges = []\n",
    "for e1 , e2 in loaded_data['PSN_EWAS'].edges(node_0.name) :  # Get the edges of this node in the networkx graph\n",
    "    node_0_edges.append(e2)\n",
    "    \n",
    "'''\n",
    "###########################################################################\n",
    "Note : DGL will reorder the nodes if integers are present in the node name\n",
    "###########################################################################\n",
    "'''\n",
    "\n",
    "sorted(list(phenotypes.iloc[g.ndata['idx'].numpy()].iloc[g.out_edges(0)[1].numpy()].index)) == sorted(node_0_edges) # Check if the edges match between the networkx and dgl graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cb887",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Graph Convolutional Network </font>\n",
    "\n",
    "Here we will specify the layers of the GCN model using the [GraphConv](https://docs.dgl.ai/en/2.0.x/generated/dgl.nn.pytorch.conv.GraphConv.html) class from DGL.\n",
    "        \n",
    "See [Object Oriented Programming](https://www.w3schools.com/python/python_classes.asp) for more information on building classes in python.\n",
    "\n",
    "See [GraphConv](https://docs.dgl.ai/en/2.0.x/generated/dgl.nn.pytorch.conv.GraphConv.html) for more documentation on the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim,  hidden_feats, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.gcnlayers = nn.ModuleList()\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        '''\n",
    "        #################################################\n",
    "                       YOUR CODE HERE\n",
    "        #################################################\n",
    "        The GraphConv class takes in the following arguments:\n",
    "            1. input_dim : The number of input features\n",
    "            2. output_dim : The number of output features\n",
    "            3. activation : The activation function to be used after the layer (we will stick to the default which is None)\n",
    "\n",
    "        We will create a list of GraphConv layers with the following dimensions:\n",
    "            1. input_dim -> hidden_feats[0]\n",
    "            2. hidden_feats[0] -> hidden_feats[1]\n",
    "            3. hidden_feats[1] -> num_classes\n",
    "\n",
    "        Where hidden_feats is a list of the number of hidden features in each layer which we will specify later\n",
    "        '''    \n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including the input layer)\n",
    "        \n",
    "        '''\n",
    "        #################################################\n",
    "                       YOUR CODE HERE\n",
    "        #################################################\n",
    "        \n",
    "        Here we will define the forward pass of the GCN model. \n",
    "        \n",
    "        The forward pass will consist of the following steps:\n",
    "            1. Pass the input features through the first GraphConv layer\n",
    "            2. Apply dropout on the output of the first layer & ReLU activation function hint use F.relu\n",
    "            3. Pass the output of the first layer through the second GraphConv layer\n",
    "            4. Apply dropout on the output of the first layer & ReLU activation function hint use F.relu\n",
    "            5. Pass the output of the second layer through the third GraphConv layer\n",
    "            6. Return the output of the third layer\n",
    "        '''\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba0721",
   "metadata": {},
   "source": [
    "### <font color='darkblue'> Training & Evaluation </font>\n",
    "\n",
    "We need to define training and evaluation functions for our GCN model. The code provided follows the outline defined in Chapter 6 of [Hamilton's book](https://www.cs.mcgill.ca/~wlh/grl_book/) for training GNN's. \n",
    "\n",
    "The code below is fully operational and provided for practical demonstration. Similar examples of GNN training functions are provided by DGL in their [Blitz Introduction](https://docs.dgl.ai/tutorials/blitz/index.html)\n",
    "\n",
    "In these functions we implement [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for multi-class classification. We use the [Adam Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) to update the model parameters and we use the [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html) function to decay the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ac6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve , average_precision_score , recall_score ,  PrecisionRecallDisplay\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(g, h, train_split , val_split , device ,  model , labels , epochs , lr):\n",
    "    # loss function, optimizer and scheduler\n",
    "    loss_fcn = nn.CrossEntropyLoss() # [Cross Entropy Loss for multi-class classification](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr , weight_decay=1e-4) # Optimizer to update model parameters [Adam Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8) # Scheduler function to decay the learning rate [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss   = []\n",
    "    train_acc  = []\n",
    "    val_acc    = []\n",
    "    \n",
    "    # training loop\n",
    "    epoch_progress = tqdm(total=epochs, desc='Loss : ', unit='epoch') # tqdm progress bar for epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "\n",
    "        logits  = model(g, h) # Get the logits from the model. Logits are analagous to the node features estimates obtained during the karate club toy example\n",
    "\n",
    "        loss = loss_fcn(logits[train_split], labels[train_split].float()) # Calculate the loss i.e. how far off the model is from the true labels\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad() # Zero the gradients before backpropagation\n",
    "        loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Update the model parameters based on the gradients\n",
    "\n",
    "        scheduler.step() # Step the scheduler to decay the learning rate\n",
    "        \n",
    "        if (epoch % 5) == 0 :\n",
    "            \n",
    "            _, predicted = torch.max(logits[train_split], 1) # Get the predicted labels from the model\n",
    "            _, true = torch.max(labels[train_split] , 1) # Get the true labels\n",
    "            train_acc.append((predicted == true).float().mean().item()) # Calculate the accuracy of the model on the training set\n",
    "\n",
    "            valid_loss , valid_acc , *_ = evaluate(val_split, device, g , h, model , labels) # Evaluate the model on the validation set\n",
    "            val_loss.append(valid_loss.item())\n",
    "            val_acc.append(valid_acc)\n",
    "            \n",
    "            epoch_desc = (\n",
    "                \"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n",
    "                    epoch, np.mean(train_loss[-5:]) , np.mean(train_acc[-5:]), np.mean(val_acc[-5:])\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            epoch_progress.set_description(epoch_desc) # Update the progress bar description\n",
    "            epoch_progress.update(5) # Update the progress bar by 5 epochs\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    fig1 , ax1 = plt.subplots(figsize=(6,4))\n",
    "    ax1.plot(train_loss  , label = 'Train Loss')\n",
    "    ax1.plot(range(5 , len(train_loss)+1 , 5) , val_loss  , label = 'Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot the training and validation accuracy\n",
    "    fig2 , ax2 = plt.subplots(figsize=(6,4))\n",
    "    ax2.plot(train_acc  , label = 'Train Accuracy')\n",
    "    ax2.plot(val_acc  , label = 'Validation Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Close tqdm for epochs\n",
    "    epoch_progress.close()\n",
    "\n",
    "    return fig1 , fig2\n",
    "\n",
    "# Define a function to evaluate the model on the validation and test set \n",
    "def evaluate(split, device, g , h, model , labels):\n",
    "    model.eval() # Set the model to evaluation mode i.e. turn off dropout etc.. \n",
    "    loss_fcn = nn.CrossEntropyLoss() # Loss function for multi-class classification [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "    acc = 0\n",
    "    \n",
    "    with torch.no_grad() :  # Turn off gradient computation for evaluation\n",
    "        logits = model(g, h) # Get the logits from the model\n",
    "        \n",
    "        loss = loss_fcn(logits[split], labels[split].float()) # Calculate the loss on the validation or test set\n",
    "\n",
    "        _, predicted = torch.max(logits[split], 1) # Get the predicted labels\n",
    "        _, true = torch.max(labels[split] , 1) # Get the true labels\n",
    "        acc = (predicted == true).float().mean().item() # Calculate the accuracy of the model\n",
    "        \n",
    "        logits_out = logits[split].cpu().detach().numpy()  # Calculate the loss i.e. how far off the model is from the true labels\n",
    "        binary_out = (logits_out == logits_out.max(1).reshape(-1,1))*1 # Convert the logits to binary predictions\n",
    "        \n",
    "        labels_out = labels[split].cpu().detach().numpy() # Get the true labels\n",
    "        \n",
    "        PRC =  average_precision_score(labels_out , binary_out , average=\"weighted\") # Calculate the precision\n",
    "        SNS = recall_score(labels_out , binary_out , average=\"weighted\") # Calculate the recall\n",
    "        F1 = 2*((PRC*SNS)/(PRC+SNS)) # Calculate the F1 score\n",
    "        \n",
    "    \n",
    "    return loss , acc , F1 , PRC , SNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871ce31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#################################################\n",
    "               YOUR CODE HERE\n",
    "#################################################\n",
    "\n",
    "Here we are going to prepare our data for input to the \n",
    "Graph Convoloution Network\n",
    "\n",
    "1. Move the graph to the relevant device - GPU/CPU\n",
    "2. Get the node labels from the meta data, order by network nodes and reset the index\n",
    "3. Specify the input shape to the model from the shape of the input data\n",
    "4. One hot encode the node labels using torch.nn.functional.one_hot()\n",
    "\n",
    "'''\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu') # Set the device to GPU if available else CPU\n",
    "\n",
    "g = g.to(device) # Move the graph to the device\n",
    "\n",
    "node_subjects =  # Get node target labels from phenoteypes meta data \n",
    "node_subjects.name = 'Smoking'\n",
    "\n",
    "GCN_input_shapes = .shape[] # Get the input shape of the GCN model\n",
    "\n",
    "labels = F.one_hot() # Encode the labels as one hot vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d6a12-7222-4087-a362-75926aae8197",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eb431-92a0-4c09-9178-7e7246309f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics = []\n",
    "logits = np.array([])\n",
    "labels_all = np.array([])\n",
    "\n",
    "train_tmp_index , test_index = train_test_split(                    # Split the data into a temporary training and test set\n",
    "    node_subjects.index, train_size=0.6, stratify=node_subjects\n",
    "    )\n",
    "train_index , val_index = train_test_split(                         # Split the temporary training set into a training and validation set\n",
    "    train_tmp_index, train_size=0.8, stratify=node_subjects.loc[train_tmp_index]\n",
    "    )\n",
    "\n",
    "model = GCN(GCN_input_shapes, hidden_feats=[128 , 32], num_classes=len(node_subjects.unique())).to(device) # Create the GCN model\n",
    "print(model)\n",
    "print(g)\n",
    "\n",
    "loss_plot = train(g, g.ndata['feat'] , train_index , val_index , device ,  model , labels , 1000 , 1e-3) # Train the model\n",
    "plt.show()\n",
    "\n",
    "test_output_metrics = evaluate(test_index , device , g , g.ndata['feat'] , model , labels ) # Evaluate the model on the test set\n",
    "\n",
    "# Print the test accuracy and F1 score\n",
    "print(\n",
    "    \"GNN Model | Test Accuracy = {:.4f} | F1 = {:.4f} |\".format(\n",
    "     test_output_metrics[1] , test_output_metrics[2] )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc368c0-1633-4880-9018-896faa7f9ed6",
   "metadata": {},
   "source": [
    "### We are going to compare our model to a Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38418729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # Import the logistic regression model from sklearn\n",
    "\n",
    "y_vals = labels.detach().cpu().numpy() # Get the labels as numpy arrays\n",
    "input_data = g.ndata['feat'].detach().cpu().numpy() # Get the input features as numpy arrays\n",
    "\n",
    "glmnet = LogisticRegression(penalty='l1' , solver='saga', max_iter = 1000 ) # Create a logistic regression model with L1 regularization\n",
    "glmnet.fit(input_data[train_index] , y_vals[train_index][: , 0]) # Fit the logistic regression model\n",
    "glmnet_acc = glmnet.score(input_data[test_index] , y_vals[test_index][: , 0]) # Get the accuracy of the logistic regression model on the test set\n",
    "print(f'Baseline Logistic Regression ACC : {glmnet_acc}') # Print the accuracy of the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0abeb",
   "metadata": {},
   "source": [
    "Our model does not outperform a simple linear regression model. This suggests that the gloabal relationships in the input data are stronger than the local relationships of our network\n",
    "\n",
    "i.e. The local structure enforced by our network is less informative that learning from all features at once\n",
    "\n",
    "In the next part, we will exhibit the predictive power of informative local structure by making a more informative network created from more than one modality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Barry ISMB Env",
   "language": "python",
   "name": "bryanismb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}